{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375eea5-daeb-4e21-aa6e-1ddc24e28745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests beautifulsoup4 sentence-transformers faiss-cpu transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ca148d-4cfe-4c58-baf7-f2ce02f8115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Extract meaningful text (ignoring scripts, styles)\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    text = \" \".join(soup.stripped_strings)  # Get clean text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4192a71e-c8d6-4271-8adc-0e031caea64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\anaconda3\\envs\\genai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load embedding model\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embeddings(text_chunks):\n",
    "    embeddings = embed_model.encode(text_chunks)\n",
    "    return np.array(embeddings, dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d10304-d4aa-45eb-ab5a-5dbff99b144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating FAISS index for fast search\n",
    "def create_faiss_index(embeddings):\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ebfa5f-4d1e-40f0-8b2a-96e8040412f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve Relevant Text from FAISS\n",
    "\n",
    "##here Retrieval: Extracting Relevant Information from a Website\n",
    "\n",
    "def search_faiss(query, index, text_chunks):\n",
    "    query_embedding = embed_model.encode([query]).astype(\"float32\")\n",
    "    _, I = index.search(query_embedding, k=3)  # Retrieve top 3 relevant chunks\n",
    "    return [text_chunks[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59250a45-bf19-4be8-bc78-ebc96f48c1c1",
   "metadata": {},
   "source": [
    "Step 1: The user asks a question.\n",
    "Step 2: The query is converted into an embedding using SentenceTransformers.\n",
    "Step 3: FAISS retrieves the top 3 most relevant text chunks from the website.\n",
    "\n",
    "Why This is \"Retrieval\"?\n",
    "Instead of using all the website text, only the most relevant parts are retrieved and passed to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e00a2-8c1e-4112-bb8a-3580d5b5b90d",
   "metadata": {},
   "source": [
    "Step 4: The retrieved relevant text is injected into the prompt as <context>.\n",
    "Step 5: The user question (query) is included below it.\n",
    "Step 6: The LLM only generates a response using the given context.\n",
    "Why This is \"Augmentation\"?\n",
    "By including relevant text from FAISS inside <context>, the AI is forced to use external knowledge rather than relying only on pre-trained data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe708ead-ce27-4ac6-b4d4-c98a6e12673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make use of chatgpt to get a prompt like below heheehh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd074b6-f426-4ea7-9d47-73beb1c921c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "def generate_response(context, query):\n",
    "    HF_API_KEY = \"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\"\n",
    "    model = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "    API_URL = f\"https://api-inference.huggingface.co/models/{model}\"\n",
    "    \n",
    "    # Improved prompt with flexible formatting guidance\n",
    "    prompt = f\"\"\"<system>\n",
    "You are a helpful AI assistant that provides accurate, comprehensive, and natural responses.\n",
    "Answer the question based solely on the information provided in the context.\n",
    "If the context doesn't contain enough information, state \"This information is not provided in the context.\"\n",
    "Format your response in the most appropriate way for the question - use paragraphs for explanations, \n",
    "bullet points for lists, and direct answers for simple questions.\n",
    "</system>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<user>\n",
    "{query}\n",
    "</user>\n",
    "\n",
    "<assistant>\n",
    "\"\"\"\n",
    "    \n",
    "    # Enhanced parameters for natural responses\n",
    "    headers = {\"Authorization\": f\"Bearer {HF_API_KEY}\"}\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"return_full_text\": False,\n",
    "            \"temperature\": 0.2,           # Slightly higher for more natural responses\n",
    "            \"top_p\": 0.9,                 \n",
    "            \"top_k\": 50,\n",
    "            \"repetition_penalty\": 1.15,\n",
    "            \"do_sample\": True,\n",
    "            \"stop\": [\"</assistant>\", \"<user>\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, json=payload) #generation \n",
    "\n",
    "    \n",
    "    try:\n",
    "        result = response.json()\n",
    "        if isinstance(result, list) and len(result) > 0 and \"generated_text\" in result[0]:\n",
    "            generated_text = result[0][\"generated_text\"].strip()\n",
    "            \n",
    "            # Clean the response of any system markers\n",
    "            if \"</assistant>\" in generated_text:\n",
    "                generated_text = generated_text.split(\"</assistant>\")[0].strip()\n",
    "                \n",
    "            return generated_text\n",
    "        else:\n",
    "            return \"Error: Unexpected response format from the model.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: Unable to generate a response. {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7bafbb-b9d2-4c2a-ba55-8de54e18f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit UI\n",
    "st.set_page_config(page_title=\"Chat with Websites\", page_icon=\"ü§ñ\")\n",
    "st.title(\"Chat with Websites - Interactive Chat\")\n",
    "\n",
    "# Sidebar for website input\n",
    "st.sidebar.header(\"Settings\")\n",
    "url = st.sidebar.text_input(\"Enter Website URL\")\n",
    "\n",
    "if url:\n",
    "    with st.spinner(\"Extracting website content...\"):\n",
    "        text = extract_text_from_url(url)\n",
    "        sentences, embeddings = get_text_embeddings(text)\n",
    "        index = create_faiss_index(embeddings)\n",
    "\n",
    "    # Initialize chat history\n",
    "    if \"chat_history\" not in st.session_state:\n",
    "        st.session_state.chat_history = [\n",
    "            {\"role\": \"AI\", \"content\": \"Hello! I'm your AI assistant. How can I help you?\"}\n",
    "        ]\n",
    "\n",
    "    # User input\n",
    "    user_query = st.chat_input(\"Type your message here...\")\n",
    "    \n",
    "    if user_query:\n",
    "        with st.spinner(\"Searching for relevant content...\"):\n",
    "            relevant_context = search_faiss(user_query, index, sentences)\n",
    "\n",
    "        with st.spinner(\"Generating response...\"):\n",
    "            answer = generate_response(\"\\n\".join(relevant_context), user_query)\n",
    "\n",
    "        # Update chat history\n",
    "        st.session_state.chat_history.append({\"role\": \"Human\", \"content\": user_query})\n",
    "        st.session_state.chat_history.append({\"role\": \"AI\", \"content\": answer})\n",
    "\n",
    "    # Display chat messages\n",
    "    for msg in st.session_state.chat_history:\n",
    "        with st.chat_message(\"AI\" if msg[\"role\"] == \"AI\" else \"Human\"):\n",
    "            st.write(msg[\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725c22cf-22d7-4920-93cb-3ecc778d309a",
   "metadata": {},
   "source": [
    "reference:1. https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3?inference_provider=together&language=python\n",
    "          2. https://www.youtube.com/watch?v=bupx08ZgSFg\n",
    "          3. https://github.com/codebasics/project-genai-cold-email-generator\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "441ac0cd-5b56-4c35-9a82-d48149d8ffa9",
   "metadata": {},
   "source": [
    "üìå How It Works\n",
    "1Ô∏è‚É£ User Input: The user asks a question.\n",
    "2Ô∏è‚É£ Website Scraping: The chatbot extracts text from a given website.\n",
    "3Ô∏è‚É£ Embedding Generation: The text is converted into numerical representations using Sentence Transformers.\n",
    "4Ô∏è‚É£ FAISS Indexing & Retrieval: The most relevant information is retrieved from the website‚Äôs content.\n",
    "5Ô∏è‚É£ Context Injection: The retrieved content is inserted into the prompt.\n",
    "6Ô∏è‚É£ LLM Response Generation: Mistral-7B processes the question + retrieved content to generate an accurate response.\n",
    "7Ô∏è‚É£ User Interaction: The chatbot remembers conversation history for better follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5eed41-c752-46b8-b8cc-b2296ca12412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
